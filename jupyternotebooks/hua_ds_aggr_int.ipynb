{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/opt/helios',\n",
       " '/opt/helios/bin',\n",
       " '/opt/helios/protoutils',\n",
       " '/opt/helios/spark/python',\n",
       " '/opt/helios/spark/python/lib/py4j-0.10.7-src.zip',\n",
       " '/usr/lib64/python36.zip',\n",
       " '/usr/lib64/python3.6',\n",
       " '/usr/lib64/python3.6/lib-dynload',\n",
       " '',\n",
       " '/usr/lib64/python3.6/site-packages',\n",
       " '/usr/lib/python3.6/site-packages',\n",
       " '/usr/lib/python3.6/site-packages/IPython/extensions',\n",
       " '/root/.ipython',\n",
       " '/mnt/code/helios/app',\n",
       " '/mnt/code/helios/app/common',\n",
       " '/mnt/code/helios/app/protoutils']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/mnt/code/helios/app')\n",
    "sys.path.append('/mnt/code/helios/app/common')\n",
    "sys.path.append('/mnt/code/helios/app/protoutils')\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import time\n",
    "\n",
    "import pyspark.sql.functions as func\n",
    "from pyspark.sql.types import IntegerType, StringType, StructType, StructField\n",
    "\n",
    "from common.log import INFO, ERROR\n",
    "from spark_utils.spark_base import SparkBase\n",
    "from spark_utils.time_utils import parse_time\n",
    "from spark_utils.log_parser import filter_bro_logs, alter_col_name, extract_data_to_cs\n",
    "from spark_utils.data_io_utils import load_kafka_batch, commit_kafka_offset, commit_log_time, write_data_cassandra\n",
    "from spark_utils.dsaggr_process import aggr_mssql, aggr_mysql, dsaggr_process_sql, dsaggr_process_form,\\\n",
    "                                       aggr_ds_servers, aggr_ds_services\n",
    "from spark_utils.ddn_config import LOGS_SCHEMA, MULTIHOME_DETECTION_WINDOW_LENGTH_DAYS\n",
    "from spark_utils.constants import LOCAL_DATA_PATH, EXPECTED_QPS, NUM_ENTRIES_PER_PARTITION\n",
    "from spark_utils.dataframe_utils import create_empty_dataframe\n",
    "from spark_utils.multihome_detector import MultihomeDetector\n",
    "\n",
    "import tags_pb2 as TAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparkRawDataProcessor(SparkBase):\n",
    "    '''\n",
    "    This spark jobs serves as the raw log parser or processor for all following jobs.\n",
    "    It reads data from upstream (Kafka), then process the raw data, and prepares them for downstream jobs.\n",
    "    '''\n",
    "    def output_data(self, data):\n",
    "        res_df_dict = dict()\n",
    "        new_partition_number = int(EXPECTED_QPS * 3600 / NUM_ENTRIES_PER_PARTITION) + 1\n",
    "        for topic, _ in LOGS_SCHEMA.items():\n",
    "            INFO(\"processing topic %s\" % topic)\n",
    "            field_list = LOGS_SCHEMA[topic]\n",
    "            source_schema = [StructField(field, StringType(), True)\n",
    "                             for field in field_list]\n",
    "            json_schema = StructType(source_schema)\n",
    "            topic_df = data.filter(func.col('topic') == topic) \\\n",
    "                .withColumn(\"value\", func.from_json(\"value\", json_schema)) \\\n",
    "                .select('topic', 'value.*')\n",
    "            topic_df = alter_col_name(topic_df)\n",
    "            topic_df = parse_time(topic_df)\n",
    "            topic_df = filter_bro_logs(topic_df, topic)\n",
    "            topic_df.repartition(new_partition_number) \\\n",
    "                .write \\\n",
    "                .partitionBy('topic', 'year', 'month', 'day', 'hour') \\\n",
    "                .parquet(LOCAL_DATA_PATH, mode='append')\n",
    "            res_df_dict[topic] = topic_df\n",
    "            self.increment(topic + '.count', topic_df.count())\n",
    "        return res_df_dict\n",
    "\n",
    "    def output_data_stream(self, data):\n",
    "        res_df_dict = dict()\n",
    "        new_partition_number = int(EXPECTED_QPS * 3600 / NUM_ENTRIES_PER_PARTITION) + 1\n",
    "        for topic, _ in LOGS_SCHEMA.items():\n",
    "            # this is to throttle connections by Spark Structured Streaming\n",
    "            time.sleep(10)\n",
    "\n",
    "            INFO(\"processing topic %s\" % topic)\n",
    "            field_list = LOGS_SCHEMA[topic]\n",
    "            source_schema = [StructField(field, StringType(), True)\n",
    "                             for field in field_list]\n",
    "            json_schema = StructType(source_schema)\n",
    "            topic_df = data.filter(func.col('topic') == topic) \\\n",
    "                .withColumn(\"value\", func.from_json(\"value\", json_schema)) \\\n",
    "                .select('topic', 'value.*')\n",
    "            topic_df = alter_col_name(topic_df)\n",
    "            topic_df = parse_time(topic_df)\n",
    "            topic_df = filter_bro_logs(topic_df, topic)\n",
    "            query = topic_df.repartition(new_partition_number) \\\n",
    "                            .writeStream \\\n",
    "                            .queryName(topic) \\\n",
    "                            .format(\"parquet\") \\\n",
    "                            .option(\"checkpointLocation\", \"/mnt/\" + 'check_point/' + topic) \\\n",
    "                            .option(\"Trigger interval\", \"60s\") \\\n",
    "                            .partitionBy('year', 'month', 'day', 'hour') \\\n",
    "                            .option('path', LOCAL_DATA_PATH + 'topic=' + topic) \\\n",
    "                            .start()\n",
    "        return res_df_dict\n",
    "\n",
    "    def save_to_cs(self, res_df_dict):\n",
    "        for topic, topic_df in res_df_dict.items():\n",
    "            if topic_df is None or topic_df.count() == 0:\n",
    "                continue\n",
    "            INFO(\"saving topic %s\" % topic)\n",
    "            try:\n",
    "                extract_data_to_cs(topic_df, topic)\n",
    "                # TODO: turn on the counter when POC starts\n",
    "                # self.increment(topic + '.count', topic_df.count())\n",
    "            except Exception:\n",
    "                cnt = topic_df.count()\n",
    "                ERROR(\"Error saivng data for %s with count %d\" % (topic, cnt))\n",
    "                if cnt > 0:\n",
    "                    sample_rate = 10.0 / cnt if cnt >= 10 else 1.0\n",
    "                    error_rows = topic_df.sample(False, sample_rate).collect()\n",
    "                    for row in error_rows:\n",
    "                        ERROR(str(row.asDict()))\n",
    "                continue\n",
    "\n",
    "    def output_storage_info(self, output_df_dict):\n",
    "        mssql_join_df = aggr_mssql(output_df_dict)\n",
    "        if mssql_join_df is not None:\n",
    "            mssql_df = dsaggr_process_sql(mssql_join_df)\n",
    "            #write_data_cassandra(mssql_df, 'data', 'storageinfo')\n",
    "\n",
    "        mysql_join_df = aggr_mysql(output_df_dict)\n",
    "        if mysql_join_df is not None:\n",
    "            mysql_df = dsaggr_process_sql(mysql_join_df)\n",
    "            #write_data_cassandra(mysql_df, 'data', 'storageinfo')\n",
    "\n",
    "        form_df_network = dsaggr_process_form(output_df_dict, 'bro')\n",
    "        form_df_network.show()\n",
    "        if form_df_network is not None:\n",
    "            self.increment('storageinfo.filesystem.invalid.storagetype.count',\n",
    "                           form_df_network.filter(func.col('storage_type') == TAGS.DB_TYPE_DEFAULT).count())\n",
    "            write_data_cassandra(form_df_network, 'data', 'storageinfo')\n",
    "\n",
    "        form_df_scanner = dsaggr_process_form(output_df_dict, 'scanner')\n",
    "        form_df_scanner.show()\n",
    "        if form_df_scanner is not None:\n",
    "            self.increment('scannerform.filesystem.invalid.storagetype.count',\n",
    "                           form_df_scanner.filter(func.col('storage_type') == TAGS.DB_TYPE_DEFAULT).count())\n",
    "            write_data_cassandra(form_df_scanner, 'data', 'scannerform')\n",
    "\n",
    "    def output_dsaggr_info(self):\n",
    "        df_ds_servers = aggr_ds_servers()\n",
    "        write_data_cassandra(df_ds_servers, 'api', 'ds_servers')\n",
    "        df_ds_services = aggr_ds_services()\n",
    "        df_ds_services.show()\n",
    "        write_data_cassandra(df_ds_services, 'api', 'ds_services')\n",
    "\n",
    "    def output_multihome_info(self,\n",
    "                              detection_log_end_time,\n",
    "                              detection_window_length_days=MULTIHOME_DETECTION_WINDOW_LENGTH_DAYS):\n",
    "        '''\n",
    "        output multihome related info to cassandra.\n",
    "        The multihome instances will be detected in time period\n",
    "        [`detection_log_end_time` - `detection_window_length_days`, `detection_log_end_time).\n",
    "        '''\n",
    "\n",
    "        detection_log_start_time = detection_log_end_time - datetime.timedelta(days=detection_window_length_days)\n",
    "\n",
    "        def get_multihome_info_detected(log_start_time, log_end_time):\n",
    "            '''\n",
    "            Return a Spark dataframe [ip: string, alter_ip: string] representing the\n",
    "            multihome instances detected in time range [`log_start_time`, `log_start_time`).\n",
    "            '''\n",
    "            if MULTIHOME_DETECTION_WINDOW_LENGTH_DAYS is None:\n",
    "                schema = StructType([\n",
    "                    StructField('app_name', StringType(), False),\n",
    "                    StructField('frontend_ip', StringType(), False),\n",
    "                    StructField('frontend_port', IntegerType(), False),\n",
    "                    StructField('backend_ip', StringType(), False),\n",
    "                    StructField('datastore_ip', StringType(), False),\n",
    "                    StructField('datastore_port', IntegerType(), False),\n",
    "                    StructField('storage_type', IntegerType(), False)])\n",
    "                return create_empty_dataframe(schema)\n",
    "\n",
    "            multihome_df = MultihomeDetector.run_from_raw_tables(log_start_time, log_end_time)\n",
    "            return multihome_df\n",
    "        # Currently only support detected multihome info.\n",
    "        # TODO add a mechanism to include hard-coded known multihome instances.\n",
    "        multihome_info_detected = get_multihome_info_detected(detection_log_start_time,\n",
    "                                                              detection_log_end_time)\n",
    "        today = datetime.date.today()\n",
    "        today = datetime.datetime(today.year, today.month, today.day)\n",
    "\n",
    "        appended_df = (multihome_info_detected\n",
    "                       .withColumn('run_date', func.lit(today))\n",
    "                       .withColumn('log_start_time', func.lit(detection_log_start_time))\n",
    "                       .withColumn('log_end_time', func.lit(detection_log_end_time)))\n",
    "        write_data_cassandra(appended_df, 'data', 'detected_multihome', mode='overwrite')\n",
    "        # Return dataframe mainly for trouble-shooting purpose.\n",
    "        return appended_df\n",
    "\n",
    "    def entry(self):\n",
    "        data = load_kafka_batch()\n",
    "        if data.count() == 0:\n",
    "            # No data loaded. Abort.\n",
    "            return\n",
    "\n",
    "        output_df_dict = self.output_data(data)\n",
    "\n",
    "        # The following section is batch mode\n",
    "        INFO(\"Saving to CS\")\n",
    "        self.save_to_cs(output_df_dict)\n",
    "\n",
    "        # Output storage info\n",
    "        INFO(\"Saving to Storage Info\")\n",
    "        self.output_storage_info(output_df_dict)\n",
    "\n",
    "        # Output dsaggr info\n",
    "        INFO(\"Saving to DSAGGR Info\")\n",
    "        self.output_dsaggr_info()\n",
    "\n",
    "        # Output multihome info\n",
    "        INFO(\"Saving to Multihome Info\")\n",
    "        max_log_time = (data\n",
    "                        .agg(func.max('timestamp').alias('max_time'))\n",
    "                        .collect()[0]\n",
    "                        .max_time)\n",
    "        self.output_multihome_info(detection_log_end_time=max_log_time)\n",
    "\n",
    "        # commit kafka offset and log_time only when all computations succeeded\n",
    "        #TODO: Currently there is a low chance that when offset gets committed, log_time does not\n",
    "        # To fix it, we will need a checkpoint system which does not rely solely on Cassandra data store.\n",
    "        # It would be most reliable to store the checkpoints (offset, log_time) on a redundant file system\n",
    "        # like HDFS (on-prem) or S3 (cloud). For now, we will use CS as a short-term solution and see how it goes.\n",
    "        commit_kafka_offset(data)\n",
    "        commit_log_time(data, table='kafka_log_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_raw_data_processor = SparkRawDataProcessor(cs_cluster=\"192.168.7.51\", cs_consistency_level='LOCAL_ONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_kafka_batch(kafka_host=\"192.168.7.51\", kafka_offset=None, topics=[\"pii_ie\"])\n",
    "output_df_dict = spark_raw_data_processor.output_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------+--------------------+--------------------+--------------------+--------------------+------+-------------+------+-------------+------+-----------+--------------------+---------+-------+---------+--------------------+--------------------+------------+----------+-------------------+----+----+-----+----+\n",
      "| topic|@timestamp|@version|       timestamp_utc|  processed_filename|       pii_json_list|                sha1|source|       orig_h|orig_p|       resp_h|resp_p|source_uuid|             ie_time|ie_exists|service| smb_name|            smb_path|            filename|storage_type|        ts|       timestamp_dt|hour| day|month|year|\n",
      "+------+----------+--------+--------------------+--------------------+--------------------+--------------------+------+-------------+------+-------------+------+-----------+--------------------+---------+-------+---------+--------------------+--------------------+------------+----------+-------------------+----+----+-----+----+\n",
      "|pii_ie|      null|    null|2019-02-23 03:03:...|/opt/bro/logs/bro...|b'\\n\\xaa\\x03\\n&\\x...|50ac3a7fc58a4ccfe...|   bro|192.168.8.230| 35922|192.168.8.134|   445|           |2019-02-23 03:03:...|    false|    SMB|form2.pdf|\\\\SERVERNAME\\anon...|/opt/bro/logs/bro...|        null|1550891002|2019-02-23 03:03:22|   3|  23|    2|2019|\n",
      "|pii_ie|      null|    null|                null|                null|                null|                null|  null|         null|  null|         null|  null|       null|                null|     null|   null|     null|                null|                null|        null|      null|               null|null|null| null|null|\n",
      "|pii_ie|      null|    null|2019-02-23 03:03:...|/opt/bro/logs/bro...|b'\\n\\xf8\\x02\\n&\\x...|f22b1a97185f33349...|   bro|192.168.8.230| 35922|192.168.8.134|   445|           |2019-02-23 03:03:...|    false|    SMB|form1.pdf|\\\\SERVERNAME\\anon...|/opt/bro/logs/bro...|        null|1550890992|2019-02-23 03:03:12|   3|  23|    2|2019|\n",
      "|pii_ie|      null|    null|2019-02-23 03:03:...|/opt/bro/logs/bro...|b'\\n\\x80\\x03\\n&\\x...|e50ed6f31976f4d57...|   bro|192.168.8.230| 35922|192.168.8.134|   445|           |2019-02-23 03:03:...|    false|    SMB|form3.pdf|\\\\SERVERNAME\\anon...|/opt/bro/logs/bro...|        null|1550891012|2019-02-23 03:03:32|   3|  23|    2|2019|\n",
      "|pii_ie|      null|    null|2019-02-23 03:03:...|/opt/bro/logs/bro...|b'\\n\\x9c\\x03\\n&\\x...|602af5291773f141f...|   bro|192.168.8.230| 35922|192.168.8.134|   445|           |2019-02-23 03:03:...|    false|    SMB|form4.pdf|\\\\SERVERNAME\\anon...|/opt/bro/logs/bro...|        null|1550891022|2019-02-23 03:03:42|   3|  23|    2|2019|\n",
      "|pii_ie|      null|    null|2019-02-23 03:04:...|/opt/bro/logs/bro...|b'\\n\\xa4\\x03\\n&\\x...|0a6bc5c1639829662...|   bro|192.168.8.230| 35922|192.168.8.134|   445|           |2019-02-23 03:04:...|    false|    SMB|form6.pdf|\\\\SERVERNAME\\anon...|/opt/bro/logs/bro...|        null|1550891042|2019-02-23 03:04:02|   3|  23|    2|2019|\n",
      "+------+----------+--------+--------------------+--------------------+--------------------+--------------------+------+-------------+------+-------------+------+-----------+--------------------+---------+-------+---------+--------------------+--------------------+------------+----------+-------------------+----+----+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output_df_dict['pii_ie'].show()\n",
    "#output_df_dict['mysql'].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+----+----------+--------------------+------------+--------------------+--------------------+------+--------------------+-------------------+-------------+----------+\n",
      "|                 loc|           ip|port| file_name|          field_name|storage_type|                sha1|     entity_pii_list|source|             ie_time|      log_timestamp|       orig_h| file_path|\n",
      "+--------------------+-------------+----+----------+--------------------+------------+--------------------+--------------------+------+--------------------+-------------------+-------------+----------+\n",
      "|192.168.8.134\u0001445...|192.168.8.134| 445|/form2.pdf|          FIRST_NAME|           3|50ac3a7fc58a4ccfe...|[0A AA 03 0A 26 0...|   bro|2019-02-23 03:03:...|2019-02-23 03:03:22|192.168.8.230|/form2.pdf|\n",
      "|192.168.8.134\u0001445...|192.168.8.134| 445|/form2.pdf|           LAST_NAME|           3|50ac3a7fc58a4ccfe...|[0A AA 03 0A 26 0...|   bro|2019-02-23 03:03:...|2019-02-23 03:03:22|192.168.8.230|/form2.pdf|\n",
      "|192.168.8.134\u0001445...|192.168.8.134| 445|/form2.pdf|       DATE_OF_BIRTH|           3|50ac3a7fc58a4ccfe...|[0A AA 03 0A 26 0...|   bro|2019-02-23 03:03:...|2019-02-23 03:03:22|192.168.8.230|/form2.pdf|\n",
      "|192.168.8.134\u0001445...|192.168.8.134| 445|/form2.pdf|SOCIAL_SECURITY_N...|           3|50ac3a7fc58a4ccfe...|[0A AA 03 0A 26 0...|   bro|2019-02-23 03:03:...|2019-02-23 03:03:22|192.168.8.230|/form2.pdf|\n",
      "|192.168.8.134\u0001445...|192.168.8.134| 445|/form2.pdf|                CITY|           3|50ac3a7fc58a4ccfe...|[0A AA 03 0A 26 0...|   bro|2019-02-23 03:03:...|2019-02-23 03:03:22|192.168.8.230|/form2.pdf|\n",
      "|192.168.8.134\u0001445...|192.168.8.134| 445|/form2.pdf|               STATE|           3|50ac3a7fc58a4ccfe...|[0A AA 03 0A 26 0...|   bro|2019-02-23 03:03:...|2019-02-23 03:03:22|192.168.8.230|/form2.pdf|\n",
      "|192.168.8.134\u0001445...|192.168.8.134| 445|/form2.pdf|            ZIP_CODE|           3|50ac3a7fc58a4ccfe...|[0A AA 03 0A 26 0...|   bro|2019-02-23 03:03:...|2019-02-23 03:03:22|192.168.8.230|/form2.pdf|\n",
      "|192.168.8.134\u0001445...|192.168.8.134| 445|/form2.pdf|             ADDRESS|           3|50ac3a7fc58a4ccfe...|[0A AA 03 0A 26 0...|   bro|2019-02-23 03:03:...|2019-02-23 03:03:22|192.168.8.230|/form2.pdf|\n",
      "|192.168.8.134\u0001445...|192.168.8.134| 445|/form2.pdf|               PHONE|           3|50ac3a7fc58a4ccfe...|[0A AA 03 0A 26 0...|   bro|2019-02-23 03:03:...|2019-02-23 03:03:22|192.168.8.230|/form2.pdf|\n",
      "|192.168.8.134\u0001445...|192.168.8.134| 445|/form2.pdf|               EMAIL|           3|50ac3a7fc58a4ccfe...|[0A AA 03 0A 26 0...|   bro|2019-02-23 03:03:...|2019-02-23 03:03:22|192.168.8.230|/form2.pdf|\n",
      "|192.168.8.134\u0001445...|192.168.8.134| 445|/form1.pdf|          FIRST_NAME|           3|f22b1a97185f33349...|[0A F8 02 0A 26 0...|   bro|2019-02-23 03:03:...|2019-02-23 03:03:12|192.168.8.230|/form1.pdf|\n",
      "|192.168.8.134\u0001445...|192.168.8.134| 445|/form1.pdf|           LAST_NAME|           3|f22b1a97185f33349...|[0A F8 02 0A 26 0...|   bro|2019-02-23 03:03:...|2019-02-23 03:03:12|192.168.8.230|/form1.pdf|\n",
      "|192.168.8.134\u0001445...|192.168.8.134| 445|/form1.pdf|       DATE_OF_BIRTH|           3|f22b1a97185f33349...|[0A F8 02 0A 26 0...|   bro|2019-02-23 03:03:...|2019-02-23 03:03:12|192.168.8.230|/form1.pdf|\n",
      "|192.168.8.134\u0001445...|192.168.8.134| 445|/form1.pdf|                CITY|           3|f22b1a97185f33349...|[0A F8 02 0A 26 0...|   bro|2019-02-23 03:03:...|2019-02-23 03:03:12|192.168.8.230|/form1.pdf|\n",
      "|192.168.8.134\u0001445...|192.168.8.134| 445|/form1.pdf|               STATE|           3|f22b1a97185f33349...|[0A F8 02 0A 26 0...|   bro|2019-02-23 03:03:...|2019-02-23 03:03:12|192.168.8.230|/form1.pdf|\n",
      "|192.168.8.134\u0001445...|192.168.8.134| 445|/form1.pdf|            ZIP_CODE|           3|f22b1a97185f33349...|[0A F8 02 0A 26 0...|   bro|2019-02-23 03:03:...|2019-02-23 03:03:12|192.168.8.230|/form1.pdf|\n",
      "|192.168.8.134\u0001445...|192.168.8.134| 445|/form1.pdf|             ADDRESS|           3|f22b1a97185f33349...|[0A F8 02 0A 26 0...|   bro|2019-02-23 03:03:...|2019-02-23 03:03:12|192.168.8.230|/form1.pdf|\n",
      "|192.168.8.134\u0001445...|192.168.8.134| 445|/form1.pdf|               PHONE|           3|f22b1a97185f33349...|[0A F8 02 0A 26 0...|   bro|2019-02-23 03:03:...|2019-02-23 03:03:12|192.168.8.230|/form1.pdf|\n",
      "|192.168.8.134\u0001445...|192.168.8.134| 445|/form1.pdf|               EMAIL|           3|f22b1a97185f33349...|[0A F8 02 0A 26 0...|   bro|2019-02-23 03:03:...|2019-02-23 03:03:12|192.168.8.230|/form1.pdf|\n",
      "|192.168.8.134\u0001445...|192.168.8.134| 445|/form3.pdf|          FIRST_NAME|           3|e50ed6f31976f4d57...|[0A 80 03 0A 26 0...|   bro|2019-02-23 03:03:...|2019-02-23 03:03:32|192.168.8.230|/form3.pdf|\n",
      "+--------------------+-------------+----+----------+--------------------+------------+--------------------+--------------------+------+--------------------+-------------------+-------------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+---+----+---------+----------+------------+----+---------------+------+-------+-------------+\n",
      "|loc| ip|port|file_name|field_name|storage_type|sha1|entity_pii_list|source|ie_time|log_timestamp|\n",
      "+---+---+----+---------+----------+------------+----+---------------+------+-------+-------------+\n",
      "+---+---+----+---------+----------+------------+----+---------------+------+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_raw_data_processor.output_storage_info(output_df_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----+------------+---+------+-----+\n",
      "|           ip|port|storage_type|dbs|tables|files|\n",
      "+-------------+----+------------+---+------+-----+\n",
      "| 192.168.8.74|3306|           2|  6|   146|    0|\n",
      "|192.168.8.155|1433|           1|  3|     8|    0|\n",
      "| 192.168.8.86|3306|           2| 10|   157|    0|\n",
      "|192.168.8.134| 445|           3|  0|     0|    5|\n",
      "+-------------+----+------------+---+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_raw_data_processor.output_dsaggr_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
