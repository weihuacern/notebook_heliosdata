{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '/root/anaconda3/lib/python36.zip',\n",
       " '/root/anaconda3/lib/python3.6',\n",
       " '/root/anaconda3/lib/python3.6/lib-dynload',\n",
       " '/root/anaconda3/lib/python3.6/site-packages',\n",
       " '/root/anaconda3/lib/python3.6/site-packages/IPython/extensions',\n",
       " '/root/.ipython',\n",
       " '/mnt/code/app',\n",
       " '/mnt/code/app/protoutils']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/mnt/code/app')\n",
    "sys.path.append('/mnt/code/app/protoutils')\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/code/target\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as func\n",
    "from pyspark.sql.types import StructField, StringType, StructType\n",
    "\n",
    "from utils import log as logging\n",
    "from common.log import def_log_cates, cust_log_cates, set_log_cate\n",
    "from common.spark_utils.ddn_utils import get_service_ip, parse_time,\\\n",
    "    extract_data_to_cs, alter_col_name, filter_bro_logs\n",
    "from common.spark_utils.ddn_config import LOGS_SCHEMA\n",
    "from common.spark_utils.dsaggr_process import aggr_mssql, aggr_mysql,\\\n",
    "                                              dsaggr_process_sql, dsaggr_process_form, dsaggr_process_scannerform,\\\n",
    "                                              aggr_ds_servers, aggr_ds_services\n",
    "from common.spark_utils.constants import *\n",
    "\n",
    "print(SPARK_CODE_ROOT)\n",
    "\n",
    "def _init_spark():\n",
    "    #TODO: make the following IP configurable.\n",
    "    conf = (SparkConf()\n",
    "            .set(\"spark.cassandra.connection.host\", \"192.168.7.110, 192.168.7.111\")\n",
    "            .set(\"spark.cassandra.auth.username\", \"cassandra\")\n",
    "            .set(\"spark.cassandra.auth.password\", \"cassandra\")\n",
    "            .set(\"spark.driver.extraClassPath\", \"/opt/helios/spark/jars\"))\n",
    "    sc = SparkContext(conf=conf)\n",
    "    sc.setLogLevel(\"ERROR\")\n",
    "    sc.addPyFile(SPARK_CODE_ROOT + '/spark_utils.zip')\n",
    "    spark = SparkSession(sc)\n",
    "\n",
    "def load_kafka_data():\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    kafka_ip = get_service_ip('connector')\n",
    "    kafka_ip_port = None\n",
    "    if kafka_ip is not None:\n",
    "        kafka_ip_port = kafka_ip + \":\" + KAFKA_PORT\n",
    "        topics = ','.join(LOGS_SCHEMA.keys())\n",
    "\n",
    "    data = spark \\\n",
    "        .read \\\n",
    "        .format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", kafka_ip_port) \\\n",
    "        .option(\"kafka.partition.assignment.strategy\", \"org.apache.kafka.clients.consumer.RoundRobinAssignor\") \\\n",
    "        .option(\"subscribe\", topics) \\\n",
    "        .load()\n",
    "    data = data.selectExpr(\"CAST(key AS STRING)\",\n",
    "                           \"CAST(value AS STRING)\",\n",
    "                           \"CAST(topic AS STRING)\",\n",
    "                           \"CAST(partition AS STRING)\")\n",
    "    return data\n",
    "\n",
    "def output_data(data):\n",
    "    res_df_dict = dict()\n",
    "    new_partition_number = int(EXPECTED_QPS * 3600 / NUM_ENTRIES_PER_PARTITION) + 1\n",
    "    for topic, _ in LOGS_SCHEMA.items():\n",
    "        field_list = LOGS_SCHEMA[topic]\n",
    "        source_schema = [StructField(field, StringType(), True)\n",
    "                         for field in field_list]\n",
    "        json_schema = StructType(source_schema)\n",
    "        topic_df = data.filter(func.col('topic') == topic) \\\n",
    "                       .withColumn(\"value\", func.from_json(\"value\", json_schema)) \\\n",
    "                       .select('topic', 'value.*')\n",
    "        topic_df = alter_col_name(topic_df)\n",
    "        topic_df = parse_time(topic_df)\n",
    "        topic_df = filter_bro_logs(topic_df, topic)\n",
    "        print(\"topic %s has counts %d\" % (topic, topic_df.count()))\n",
    "        # topic_df.repartition(new_partition_number) \\\n",
    "        #         .write \\\n",
    "        #         .partitionBy('topic', 'year', 'month', 'day', 'hour') \\\n",
    "        #         .parquet(HDFS_DATA_PATH+'bro_logs', 'append')\n",
    "        res_df_dict[topic] = topic_df\n",
    "    return res_df_dict\n",
    "\n",
    "def save_to_cs(res_df_dict):\n",
    "    for topic, topic_df in res_df_dict.items():\n",
    "        if topic_df is None \\\n",
    "            or topic_df.count() == 0:\n",
    "            continue\n",
    "        extract_data_to_cs(topic_df, topic)\n",
    "\n",
    "def entry():\n",
    "    logging.setup(logfile=\"/mnt/log/spark_kafka_listener_app.log\")\n",
    "    set_log_cate(def_log_cates | cust_log_cates)\n",
    "    _init_spark()\n",
    "    data = load_kafka_data()\n",
    "    output_df_dict = output_data(data)\n",
    "    save_to_cs(output_df_dict)\n",
    "\n",
    "    #Here we start to put dsaggr data into cs\n",
    "    mssql_join_df = aggr_mssql(output_df_dict)\n",
    "    dsaggr_process_sql(mssql_join_df)\n",
    "    mysql_join_df = aggr_mysql(output_df_dict)\n",
    "    dsaggr_process_sql(mysql_join_df)\n",
    "    dsaggr_process_form(output_df_dict, 'http')\n",
    "    dsaggr_process_form(output_df_dict, 'smb_files')\n",
    "    dsaggr_process_scannerform(output_df_dict)\n",
    "    aggr_ds_servers()\n",
    "    aggr_ds_services()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o33.addFile.\n: java.io.FileNotFoundException: File file:/mnt/code/target/spark_utils.zip does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421)\n\tat org.apache.spark.SparkContext.addFile(SparkContext.scala:1529)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-159e0d778b8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mentry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-c73161b8a97c>\u001b[0m in \u001b[0;36mentry\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/mnt/log/spark_kafka_listener_app.log\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mset_log_cate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdef_log_cates\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mcust_log_cates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0m_init_spark\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_kafka_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0moutput_df_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-c73161b8a97c>\u001b[0m in \u001b[0;36m_init_spark\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetLogLevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ERROR\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddPyFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSPARK_CODE_ROOT\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/spark_utils.zip'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36maddPyFile\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    869\u001b[0m         \u001b[0mHTTP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHTTPS\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mFTP\u001b[0m \u001b[0mURI\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m         \"\"\"\n\u001b[0;32m--> 871\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    872\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# dirname may be directory or HDFS/S3 prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPACKAGE_EXTENSIONS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36maddFile\u001b[0;34m(self, path, recursive)\u001b[0m\n\u001b[1;32m    860\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m400\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m         \"\"\"\n\u001b[0;32m--> 862\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursive\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    863\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0maddPyFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o33.addFile.\n: java.io.FileNotFoundException: File file:/mnt/code/target/spark_utils.zip does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421)\n\tat org.apache.spark.SparkContext.addFile(SparkContext.scala:1529)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "entry()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
